# SupportBot Documentation Index

**Last Updated**: 2026-02-19
**Status**: Production-Ready ‚úÖ

---

## Quick Links

### Core Documentation
1. **[ALGORITHM_FLOW.md](./ALGORITHM_FLOW.md)** - Complete technical flow with actual prompts
2. **[CASE_EXAMPLES.md](./CASE_EXAMPLES.md)** - Real evaluation examples with judge outputs
3. **[FINAL_EVALUATION_REPORT.md](./FINAL_EVALUATION_REPORT.md)** - Production readiness report
4. **[DEPLOYMENT.md](./DEPLOYMENT.md)** - Oracle Cloud deployment guide
5. **[SIGNAL_REGISTRATION.md](./SIGNAL_REGISTRATION.md)** - Prevent re-linking + QR linking + readiness checklist
6. **[PRIVACY_SECURITY.md](./PRIVACY_SECURITY.md)** - Data lifecycle, privacy guarantees, admin commands

---

## Case Pipeline Architecture: B1 / B2 / B3 / SCRAG

### Overview

The bot uses a four-layer pipeline to track and resolve support cases, both from historical ingestion and from live group messages.

```
SCRAG  ‚Äî Solved Cases RAG       (ChromaDB, permanent, semantically searchable)
B1     ‚Äî Open Cases Buffer       (MySQL cases WHERE status='open', NOT in SCRAG)
B2     ‚Äî Rolling Message Buffer  (MySQL buffers table, sliding time/count window)
B3     ‚Äî Recent Solved Buffer    (solved cases whose evidence still falls in B2 window)
```

### Data Flow

```
Incoming message (live or ingest)
        ‚îÇ
        ‚ñº
  raw_messages table  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ                                                    ‚îÇ
        ‚ñº                                                    ‚îÇ
  B2: Rolling buffer                                        (evidence linking)
  (buffers table)
        ‚îÇ
        ‚ñº
  LLM: Extract case spans from B2
        ‚îÇ
        ‚îú‚îÄ‚îÄ Open case detected (problem, no solution yet)
        ‚îÇ       ‚îî‚îÄ‚îÄ‚ñ∫ B1: store in cases (status='open', in_rag=0)
        ‚îÇ                Keep messages in B2
        ‚îÇ
        ‚îî‚îÄ‚îÄ Solved case detected (problem + solution)
                ‚îî‚îÄ‚îÄ‚ñ∫ DB: insert case (status='solved')
                     SCRAG: embed + upsert to ChromaDB (in_rag=1)
                     B3: visible via get_recent_solved_cases()
                     B2: remove consumed message spans

        ‚îÇ
        ‚ñº
  Phase 2: Dynamic B1 resolution
  For each B1 (open) case in this group:
        LLM: check_case_resolved(case_problem, B2_buffer)
        ‚îÇ
        ‚îú‚îÄ‚îÄ Not resolved ‚Üí keep in B1
        ‚îî‚îÄ‚îÄ Resolved     ‚Üí update_case_to_solved()
                           embed + upsert to SCRAG (in_rag=1)
                           now visible in B3 context
```

### Answer Engine Context

When a user sends a message, the bot builds a three-layer context:

| Layer | Source | Description |
|-------|--------|-------------|
| SCRAG | ChromaDB semantic search | All-time solved cases, filtered by group |
| B3    | `get_recent_solved_cases()` | Solved cases with evidence still in the B2 window |
| B1    | `get_open_cases_for_group()` | Currently open, unresolved cases |

**Response rules:**
- SCRAG or B3 hit with solution ‚Üí synthesize direct answer + case link
- Only B1 hit ‚Üí acknowledge the issue is tracked, TAG_ADMIN
- Nothing found ‚Üí TAG_ADMIN

### DB Schema Additions

```sql
-- cases table now has:
in_rag  TINYINT(1) NOT NULL DEFAULT 0   -- 1 = indexed in ChromaDB SCRAG

-- New query functions in queries_mysql.py:
get_open_cases_for_group(db, group_id)          -- B1 lookup
get_recent_solved_cases(db, group_id, since_ts) -- B3 lookup
update_case_to_solved(db, case_id, solution)    -- B1 ‚Üí solved promotion
mark_case_in_rag(db, case_id)                   -- set in_rag=1
```

### LLM Calls per Message

| Call | Model | Purpose |
|------|-------|---------|
| `extract_case_from_buffer` | model_extract | Identify case spans in B2 |
| `make_case` | model_case | Structured case extraction (title, status, solution) |
| `check_case_resolved` | model_case | Check if B1 cases are resolved by new B2 content |
| `embed` | embedding model | Vectorise case doc for SCRAG upsert / search |
| `synthesizer` | gemini-2.0-flash | Final answer generation |

### Multimodal Support

Images attached to Signal messages are processed at ingest time:
- `ingestion.py` calls `llm.image_to_text_json()` ‚Üí JSON with `observations` + `extracted_text`
- The JSON is appended to `content_text` in `raw_messages`
- Evidence images are stored in `evidence_image_paths_json` on the case
- Case extraction and resolution LLM calls receive image bytes when available

---

### Legacy Documentation
- **[legacy/](./legacy/)** - Historical evaluation reports and analysis

---

## What's in Each Document

### ALGORITHM_FLOW.md
**Purpose**: Technical implementation reference

**Contents**:
- System architecture diagram
- Complete 3-stage pipeline (DECIDE_CONSIDER ‚Üí RETRIEVE ‚Üí RESPOND)
- **Actual prompts** from `signal-bot/app/llm/prompts.py`:
  - P_IMG_SYSTEM (image processing)
  - P_DECISION_SYSTEM (stage 1 filtering)
  - P_RESPOND_SYSTEM (stage 3 generation)
- Input/output schemas for each LLM call
- Step-by-step case flow with real examples
- Configuration parameters
- Error handling patterns

**Use When**: You need to understand HOW the system works internally

---

### CASE_EXAMPLES.md
**Purpose**: Real-world behavior examples

**Contents**:
- **Perfect responses (10/10)**: 6 detailed examples
  - EKF3 IMU0 error with image (multimodal)
  - IMX290-83 build selection
  - SoloGoodF722 support inquiry
- **Excellent responses (9/10)**: 2 examples
  - Camera FOV issue
  - Changelog query
- **Partial failures (4/10)**: 2 examples
  - Koshmarik error (relevance issue)
  - Pi Zero 2 vs Pi 4 (missed comparison)
- **Complete failures (0/10)**: 2 examples
  - No response on valid question (over-cautious)
- **Multimodal processing**: 2 success examples
- **Statement detection**: Correct silence examples
- **Noise filtering**: 100% success examples
- **Off-topic declination**: Mixed results

**Each Example Includes**:
- Full input message (with images if applicable)
- Step-by-step pipeline execution
- LLM reasoning at each stage
- Final response
- Judge evaluation with score and reasoning
- Metrics (length, accuracy, relevance, etc.)

**Use When**: You want to see HOW the bot behaves in real scenarios

---

### FINAL_EVALUATION_REPORT.md
**Purpose**: Production readiness assessment

**Contents**:
- Executive summary (85% pass rate, 93.75% on real cases)
- Detailed results breakdown by category
- Knowledge base statistics (400 messages ‚Üí 16 cases)
- Performance comparison across evaluations
- Key achievements:
  - Multimodal image support ‚úÖ
  - Statement detection ‚úÖ
  - Zero hallucinations ‚úÖ
- Response quality examples
- Failure analysis
- Production readiness checklist
- Deployment recommendations

**Use When**: You need to justify production deployment or understand system performance

---

### DEPLOYMENT.md
**Purpose**: Step-by-step deployment instructions

**Contents**:
- Oracle Cloud infrastructure setup
- Signal CLI configuration
- Redis setup for message buffering
- Environment variables
- Monitoring setup
- Troubleshooting common issues

**Use When**: You're actually deploying the system

---

## Evaluation Data Sources

All documentation is based on **real evaluation data**:

1. **Quality Evaluation** (`test/data/real_quality_eval.json`)
   - 49 scenarios (45 should_answer, 2 should_decline, 2 should_ignore)
   - 91.1% pass rate on should_answer cases
   - Average quality score: 8.91/10
   - Includes judge reasoning for each case

2. **Streaming Evaluation** (`test/data/streaming_eval/`)
   - 400 context messages used to build KB
   - 14 cases extracted from real Signal group chat
   - 75 evaluation messages tested
   - Includes full judge details

3. **Actual Signal Group Data**
   - Real Ukrainian tech support conversations
   - Group: "–¢–µ—Ö–ø—ñ–¥—Ç—Ä–∏–º–∫–∞ –ê–∫–∞–¥–µ–º—ñ—è –°—Ç–∞–±–•"
   - Topics: Drone flight controllers, ArduPilot, cameras, etc.

4. **Signal Community Forum Dataset** (Public Benchmark)
   - **Source**: https://community.signalusers.org (Category: Support)
   - **Size**: ~50 topics (demonstration), expandable to full history
   - **Format**: Reproducible public data with "accepted answers" as ground truth
   - **Scripts**: `paper/scripts/fetch_dataset.py` and `paper/scripts/process_dataset.py`
   - **Purpose**: Publicly verifiable benchmark for the paper

---

## Key Metrics Summary

| Metric | Value | Status |
|--------|-------|--------|
| **Overall Pass Rate** | 85.0% | ‚úÖ Target met (80-90%) |
| **Should Answer** | 93.75% | ‚úÖ Exceeded target |
| **Should Decline** | 50% | ‚ö†Ô∏è Acceptable (caught in Stage 3) |
| **Should Ignore** | 100% | ‚úÖ Perfect |
| **Avg Quality Score** | 9.125/10 | ‚úÖ Exceeded target (8.0+) |
| **Zero Hallucinations** | ‚úÖ Verified | ‚úÖ Critical requirement met |
| **Multimodal Support** | ‚úÖ Implemented | ‚úÖ Working |
| **Response Length** | 195 chars avg | ‚úÖ Well under 500 limit |

---

## System Capabilities

### ‚úÖ What Works Excellently

1. **Multimodal Image Processing**
   - OCR text extraction from screenshots
   - Visual observation extraction
   - Context integration with user message
   - 90%+ success rate on image-based questions

2. **Noise Filtering**
   - 100% success rate on greetings
   - 100% success rate on emoji-only messages
   - 100% success rate on acknowledgements
   - Perfect "statement vs question" detection

3. **Zero Hallucinations**
   - Never fabricates facts
   - Only responds with evidence from KB or buffer
   - Stays silent when insufficient information
   - All responses cite source evidence IDs

4. **Ukrainian Language**
   - Native-quality responses
   - Appropriate technical terminology
   - Concise and clear communication style

5. **Response Quality**
   - 53.3% perfect scores (10/10)
   - 37.8% excellent scores (9/10)
   - Average: 9.125/10
   - Average length: 195 chars (concise!)

### ‚ö†Ô∏è Known Limitations

1. **Stage 1 False Positives** (~10%)
   - Some off-topic questions pass Stage 1 filter
   - But always caught in Stage 3 (no false positives sent)
   - Wastes tokens on unnecessary retrieval

2. **Over-Cautious Stage 3** (~5%)
   - Sometimes refuses to answer valid questions
   - Happens when KB match is not exact
   - Could provide more helpful partial answers

3. **Comparison Questions** (~10% lower quality)
   - "X vs Y" questions get lower scores
   - Bot tends to focus on one option
   - Better at direct "which X?" questions

---

## How to Use This Documentation

### For Developers

1. **Understanding the code**: Read ALGORITHM_FLOW.md
2. **Testing changes**: Use CASE_EXAMPLES.md to verify behavior
3. **Debugging issues**: Check failure examples in CASE_EXAMPLES.md
4. **Deploying**: Follow DEPLOYMENT.md step-by-step

### For Evaluators

1. **Performance metrics**: See FINAL_EVALUATION_REPORT.md
2. **Real examples**: Browse CASE_EXAMPLES.md
3. **Judge criteria**: Check judge_details in examples

### For Product Managers

1. **Executive summary**: FINAL_EVALUATION_REPORT.md (first 2 pages)
2. **Success stories**: Perfect examples in CASE_EXAMPLES.md
3. **Improvement areas**: Failure analysis sections

---

## Prompt Updates

**Last Verified**: 2026-02-11

All prompts in ALGORITHM_FLOW.md are **current and match** the actual implementation in:
- `signal-bot/app/llm/prompts.py` (lines 1-160)

Prompts included:
- ‚úÖ P_IMG_SYSTEM (image extraction)
- ‚úÖ P_DECISION_SYSTEM (stage 1 filtering)
- ‚úÖ P_RESPOND_SYSTEM (stage 3 generation)
- ‚úÖ P_EXTRACT_SYSTEM (case mining)
- ‚úÖ P_CASE_SYSTEM (case structuring)

---

## Quick Reference

### File Locations

```
docs/
‚îú‚îÄ‚îÄ README.md                      ‚Üê You are here
‚îú‚îÄ‚îÄ ALGORITHM_FLOW.md              ‚Üê Technical implementation
‚îú‚îÄ‚îÄ CASE_EXAMPLES.md               ‚Üê Real evaluation examples
‚îú‚îÄ‚îÄ FINAL_EVALUATION_REPORT.md     ‚Üê Production readiness
‚îú‚îÄ‚îÄ DEPLOYMENT.md                  ‚Üê Deployment guide
‚îî‚îÄ‚îÄ legacy/                        ‚Üê Historical docs

test/data/
‚îú‚îÄ‚îÄ real_quality_eval.json         ‚Üê 49 quality scenarios
‚îî‚îÄ‚îÄ streaming_eval/
    ‚îú‚îÄ‚îÄ eval_results.json          ‚Üê 75 eval messages + judge
    ‚îú‚îÄ‚îÄ eval_summary.json          ‚Üê Aggregated metrics
    ‚îú‚îÄ‚îÄ context_kb.json            ‚Üê 14-case knowledge base
    ‚îî‚îÄ‚îÄ eval_messages_labeled.json ‚Üê Labeled test set

signal-bot/app/llm/
‚îú‚îÄ‚îÄ prompts.py                     ‚Üê All LLM prompts
‚îú‚îÄ‚îÄ client.py                      ‚Üê LLM client implementation
‚îî‚îÄ‚îÄ schemas.py                     ‚Üê Pydantic schemas
```

### Key Commands

```bash
# Run quality evaluation
RUN_REAL_LLM_TESTS=1 pytest test/test_quality_eval.py -v -s

# Run streaming evaluation
python test/run_streaming_eval.py

# Prepare evaluation dataset
python test/prepare_streaming_eval_dataset.py

# Run all unit tests
pytest test/ -v
```

---

## Status Legend

- ‚úÖ **Production-Ready**: Tested and working
- ‚ö†Ô∏è **Acceptable**: Minor issues, acceptable for production
- ‚ùå **Needs Fix**: Requires improvement before production
- üöß **In Progress**: Currently being developed

---

## Questions?

For implementation details: See ALGORITHM_FLOW.md  
For behavior examples: See CASE_EXAMPLES.md  
For deployment: See DEPLOYMENT.md  
For performance metrics: See FINAL_EVALUATION_REPORT.md

---

**Document Version**: 1.0  
**Maintainer**: AI Development Team  
**Last Updated**: 2026-02-11
