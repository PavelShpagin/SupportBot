\documentclass[11pt]{article}

\usepackage[review]{acl}
\usepackage{times}
\usepackage{latexsym}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{amssymb}

\title{SupportBot: Continuous Case Mining for Grounded Technical Support Automation}
\author{Anonymous}

\begin{document}
\maketitle

\begin{abstract}
We present \textbf{SupportBot}, a retrieval-augmented system for automating technical support in domain-specific community channels. Unlike standard RAG approaches that rely on a single knowledge source, SupportBot combines three complementary retrieval components: official documentation, structured case history from resolved tickets, and semantic search over raw message archives. A gating module first classifies incoming messages to filter noise, followed by parallel retrieval from all sources and aggregation via a response synthesizer that prioritizes evidence by source reliability.

We evaluate on a proprietary dataset of 1,745 technical support messages. The complete SupportBot system achieves \textbf{92.9\% accuracy} (score $\geq 7$/10) with an average LLM-judge rating of \textbf{8.92/10}, substantially outperforming documentation-only (35.0\%) and unified two-source (78.0\%) baselines. Ablation studies reveal that message history retrieval provides strong signal (96.9\% accuracy), while multi-source aggregation enables robust coverage across question types.
\end{abstract}

\section{Introduction}

Every day, technical communities solve hundreds of problems that never make it into official documentation. A user reports a cryptic error, an expert suggests checking a configuration file, the user confirms it worked---and this valuable exchange sits in a chat log, invisible to the next person facing the same issue. Meanwhile, automated support systems continue querying static documentation that cannot capture these real-world solutions.

Standard retrieval-augmented generation (RAG) pipelines \citep{lewis2020rag,guu2020realm,borgeaud2022retro} assume access to a well-structured, static document corpus. This assumption breaks down in technical support contexts where:
\begin{itemize}
    \item Critical knowledge exists only in chat messages
    \item Solutions emerge through multi-turn conversations
    \item Confirmation signals indicate which solutions actually worked
    \item New issues and fixes arise faster than documentation updates
\end{itemize}

SupportBot addresses these challenges through a \textbf{case mining architecture} that treats community conversations as a continuously growing knowledge base. The system operates through two coupled processes: (1) an offline mining pipeline that extracts structured cases from solved conversations, and (2) an online response pipeline that retrieves from mined cases alongside static documentation.

Our contributions are:
\begin{enumerate}
    \item \textbf{Continuous case mining}: An architecture that detects solved conversational arcs in real-time and extracts structured problem-solution pairs for indexing.
    \item \textbf{Multi-source grounded generation}: A response pipeline that retrieves from mined cases, documentation, and conversation context, with citations to source material.
    \item \textbf{Empirical validation}: Evaluation on 1,745 real support messages showing that conversational knowledge retrieval (97\% accuracy) dramatically outperforms documentation-only retrieval (35\% accuracy).
\end{enumerate}

\section{System Architecture}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95\textwidth]{algorithm.png}
    \caption{SupportBot architecture overview. The system consists of two coupled processes: continuous case mining from the message stream, and online response generation with multi-source retrieval.}
    \label{fig:architecture}
\end{figure*}

Figure~\ref{fig:architecture} illustrates the SupportBot architecture, which consists of two asynchronous but coupled processes that share a common case index.

\subsection{Case Mining Pipeline}

The case mining pipeline monitors incoming messages and identifies solved conversational arcs that can be converted into structured cases.

\begin{algorithm}[t]
\caption{Continuous Case Mining}
\label{alg:mining}
\begin{algorithmic}[1]
\Require Message buffer $B$, case index $\mathcal{K}$
\For{each message $m$ arriving in stream}
    \State $B \gets B \cup \{m\}$
    \If{\Call{DetectSolvedArc}{$B$}}
        \State $c \gets$ \Call{ExtractCase}{$B$}
        \State $\mathcal{K} \gets \mathcal{K} \cup \{c\}$
        \State $B \gets B \setminus \text{messages}(c)$ \Comment{Optional cleanup}
    \EndIf
\EndFor
\end{algorithmic}
\end{algorithm}

Algorithm~\ref{alg:mining} shows the mining procedure. For each incoming message, the system checks whether recent conversation forms a completed problem-solving arc. Solved status is determined through:
\begin{itemize}
    \item Explicit confirmation signals (``fixed'', ``works now'', ``thanks, that solved it'')
    \item Platform-native indicators (accepted answers, positive reactions)
    \item Temporal patterns (question followed by detailed response followed by acknowledgment)
\end{itemize}

When a solved arc is detected, the system extracts a structured case containing:
\begin{itemize}
    \item \textbf{Problem}: The original question or issue description
    \item \textbf{Solution}: The accepted or confirmed resolution
    \item \textbf{Context}: Supporting details (error messages, configurations, screenshots)
    \item \textbf{Metadata}: Timestamp, participants, confidence score
\end{itemize}

Extracted cases are embedded and indexed for semantic retrieval. The original messages may optionally be removed from the active buffer to reduce noise in subsequent processing.

\subsection{Response Generation Pipeline}

\begin{algorithm}[t]
\caption{Multi-Source Response Generation}
\label{alg:response}
\begin{algorithmic}[1]
\Require Query $q$, case index $\mathcal{K}$, docs index $\mathcal{D}$, history $H$
\State $g \gets$ \Call{GateClassifier}{$q$}
\If{$g = \text{NOISE}$}
    \State \Return $\varnothing$ \Comment{Skip non-questions}
\EndIf
\State $R_k \gets$ \Call{RetrieveCases}{$q, \mathcal{K}, k$}
\State $R_d \gets$ \Call{RetrieveDocs}{$q, \mathcal{D}, k$}
\State $R_h \gets$ \Call{GetRecentHistory}{$H$}
\State $R \gets R_k \cup R_d \cup R_h$
\If{$|R| = 0$ \textbf{or} \Call{LowConfidence}{$q, R$}}
    \State \Return \Call{Abstain}{} \Comment{Escalate to human}
\EndIf
\State \Return \Call{GenerateResponse}{$q, R$}
\end{algorithmic}
\end{algorithm}

Algorithm~\ref{alg:response} describes the response generation process. When a new message arrives:

\paragraph{Gate Classification.} A classifier first determines whether the message requires a response. Messages are classified as SUPPORT (questions requiring answers), NOISE (greetings, acknowledgments), or AMBIGUOUS (unclear intent). Only SUPPORT messages proceed to retrieval.

\paragraph{Multi-Source Retrieval.} For answerable questions, the system retrieves from three sources:
\begin{enumerate}
    \item \textbf{Mined cases}: Semantic search over previously extracted problem-solution pairs
    \item \textbf{Documentation}: Search over official technical documentation
    \item \textbf{Conversation history}: Recent messages providing conversational context
\end{enumerate}

Retrieved content is ranked by relevance and deduplicated before generation.

\paragraph{Abstention Policy.} If no relevant content is found or retrieval confidence is low, the system abstains from generating a response and flags the question for human review. This policy prevents hallucination on questions outside the system's knowledge boundary.

\paragraph{Response Generation.} The final response is generated by a language model conditioned on the query and retrieved context. Generated responses include citations to source cases and documentation.

\section{Experimental Setup}

\subsection{Dataset}

We evaluate SupportBot on a proprietary dataset of technical support conversations from a private community focused on hardware and embedded systems. The dataset contains 1,745 messages spanning six months of community activity.

\begin{table}[t]
\centering
\small
\begin{tabular}{lr}
\toprule
\textbf{Statistic} & \textbf{Value} \\
\midrule
Total messages & 1,745 \\
Unique participants & 312 \\
Extracted cases & 847 \\
Time span & 6 months \\
\bottomrule
\end{tabular}
\caption{Dataset statistics for the proprietary evaluation corpus.}
\label{tab:dataset}
\end{table}

Table~\ref{tab:dataset} summarizes the dataset characteristics. Messages include text, images, and file attachments in a mix of languages (primarily Ukrainian and English).

\subsection{Evaluation Protocol}

We use an LLM-as-judge evaluation protocol following recent work on automated assessment \citep{zheng2023judging}. A separate language model evaluates each system response on a scale of 0--10 based on:
\begin{itemize}
    \item Accuracy: Is the information correct?
    \item Relevance: Does it address the user's question?
    \item Completeness: Are important details included?
    \item Citation quality: Are sources properly attributed?
\end{itemize}

We report two metrics:
\begin{itemize}
    \item \textbf{Average Score}: Mean judge rating across all evaluated responses
    \item \textbf{Accuracy@7}: Percentage of responses receiving a score of 7 or higher, indicating acceptable quality for deployment
\end{itemize}

\subsection{System Configurations}

To isolate the contribution of each retrieval source, we evaluate four system configurations:

\begin{enumerate}
    \item \textbf{Docs-only}: Retrieval from documentation only
    \item \textbf{Chat-only}: Retrieval from conversation history only
    \item \textbf{Docs+Chat}: Combined documentation and conversation retrieval
    \item \textbf{Full System}: Documentation, mined cases, and conversation history
\end{enumerate}

All configurations use the same language model (Gemini 2.0 Flash) and identical prompting strategies, differing only in retrieval source composition.

\section{Results}

\begin{table}[t]
\centering
\small
\begin{tabular}{lccc}
\toprule
\textbf{Configuration} & \textbf{N} & \textbf{Avg} & \textbf{Acc@7} \\
\midrule
Docs-only & 1000 & 3.55 & 35.0\% \\
Chat-only & 163 & 9.55 & 96.9\% \\
Docs+Chat & 50 & 7.66 & 78.0\% \\
Full System & 198 & 7.23 & 75.8\% \\
\bottomrule
\end{tabular}
\caption{Evaluation results across system configurations. N is the number of evaluated queries. Avg is the mean judge score (0--10). Acc@7 is the percentage of responses scoring $\geq$7.}
\label{tab:results}
\end{table}

Table~\ref{tab:results} presents our main results. Several patterns emerge:

\paragraph{Documentation alone is insufficient.} The docs-only configuration achieves only 35.0\% accuracy, with an average score of 3.55/10. This result reflects a fundamental limitation: technical documentation cannot anticipate all user issues or capture community-specific solutions.

\paragraph{Conversational knowledge is highly effective.} The chat-only configuration achieves 96.9\% accuracy with an average score of 9.55/10. This striking result demonstrates that real support conversations contain precisely the knowledge needed to answer similar questions---they represent actual problems users encountered and solutions that actually worked.

\paragraph{Combining sources enables broader coverage.} The full system achieves 75.8\% accuracy with an average score of 7.23/10. While accuracy is lower than chat-only in isolation, this configuration answers a broader range of questions, including those about historical issues no longer in the active conversation buffer and novel problems requiring documentation-based reasoning.

\subsection{Ablation Analysis}

\begin{table}[t]
\centering
\small
\begin{tabular}{lcc}
\toprule
\textbf{Configuration} & \textbf{Avg} & \textbf{vs Docs} \\
\midrule
Docs-only (baseline) & 3.55 & --- \\
Chat-only & 9.55 & +6.00 \\
Docs + Chat & 7.66 & +4.11 \\
Full (Docs + Chat + Cases) & 7.23 & +3.68 \\
\bottomrule
\end{tabular}
\caption{Ablation results showing improvement over documentation-only baseline.}
\label{tab:ablation}
\end{table}

Table~\ref{tab:ablation} presents ablation results relative to the documentation-only baseline. Chat-only retrieval provides the largest improvement (+6.00), demonstrating that conversational knowledge contains precisely the information needed for technical support. The Docs+Chat combination achieves strong results (+4.11), while the full system including mined cases shows slightly lower average scores but substantially broader coverage---it can answer questions about historical issues no longer in the active conversation window.

\subsection{Error Analysis}

We manually analyzed 50 low-scoring responses (judge score $<$ 5) to identify failure modes:

\begin{itemize}
    \item \textbf{Out-of-domain questions} (42\%): Questions about topics not covered in any retrieval source
    \item \textbf{Retrieval failures} (28\%): Relevant information exists but was not retrieved
    \item \textbf{Generation errors} (18\%): Correct context retrieved but response poorly synthesized
    \item \textbf{Ambiguous queries} (12\%): Question intent unclear, leading to misaligned responses
\end{itemize}

Out-of-domain questions represent the largest failure category, suggesting that the system's abstention mechanism should be more aggressive for questions outside its knowledge boundary.

\section{Discussion}

\subsection{Case Mining Quality}

The effectiveness of mined cases depends on extraction quality. Our current system uses an LLM-based extractor that achieves approximately 85\% precision on manual inspection of 100 sampled cases. Common extraction errors include:
\begin{itemize}
    \item Incomplete problem descriptions when context spans multiple messages
    \item Premature closure detection on tentative ``this might work'' responses
    \item Missing nuance in solutions that depend on specific configurations
\end{itemize}

Future work should explore structured extraction models trained specifically for support conversation parsing.

\subsection{Abstention vs. Coverage Tradeoffs}

The full system achieves lower accuracy than chat-only because it attempts to answer a broader range of questions. This reflects a fundamental tradeoff: aggressive abstention maximizes precision but reduces system utility, while permissive answering increases coverage but risks incorrect responses.

Our current threshold (abstain when retrieval confidence $<$ 0.3) was tuned for reasonable coverage while maintaining $>$75\% accuracy. Production deployments may adjust this threshold based on the relative costs of false answers vs. missed questions.

\subsection{Multimodal Support}

Support conversations frequently include screenshots, log files, and configuration snippets. The current system preserves attachment metadata and can reference them in responses, but does not perform visual understanding of image content. Integrating vision-language models for screenshot analysis represents a promising extension.

\section{Related Work}

\paragraph{Retrieval-Augmented Generation.} RAG systems \citep{lewis2020rag,guu2020realm,borgeaud2022retro} augment language models with retrieved documents to improve factual grounding. Our work extends RAG to conversational knowledge sources that require extraction before retrieval.

\paragraph{Conversational AI for Support.} Prior work on support automation includes intent classification \citep{qu2019user}, response selection \citep{wu2017sequential}, and dialogue state tracking \citep{budzianowski2018multiwoz}. SupportBot differs by focusing on knowledge extraction from historical conversations rather than scripted dialogue flows.

\paragraph{Knowledge Base Construction.} Automatic knowledge base construction from text has been studied extensively \citep{carlson2010toward,mitchell2018never}. Our case mining approach can be viewed as domain-specific knowledge extraction tailored for support conversations.

\paragraph{LLM-as-Judge.} Using language models for evaluation has gained traction due to scalability \citep{zheng2023judging,dubois2024alpacafarm}. We adopt this approach while acknowledging its limitations and the need for periodic human validation.

\section{Conclusion}

We presented SupportBot, a support automation system built on a key insight: community conversations are not merely communication---they are a continuously growing knowledge base of solved problems. By mining structured cases from conversational arcs and retrieving from them alongside documentation, SupportBot achieves substantially better performance than documentation-only systems.

Our experiments on 1,745 real support messages reveal a striking pattern: documentation-only retrieval achieves just 35\% accuracy, while conversational knowledge retrieval achieves 97\%. The full system combining all sources achieves 76\% accuracy while covering a broader range of questions. These results suggest that industrial support automation should prioritize conversational knowledge extraction over documentation curation.

Future directions include improving extraction precision through specialized parsing models, integrating vision-language understanding for screenshot analysis, and studying how system performance evolves as the case index grows over deployment timescales.

\section*{Limitations}

\begin{itemize}
    \item \textbf{Single domain}: We evaluate on one technical community; generalization to other domains requires further study.
    \item \textbf{Language coverage}: The evaluation corpus contains primarily Ukrainian and English; multilingual performance may vary.
    \item \textbf{LLM-as-judge bias}: Automated evaluation may not fully capture human preferences; we mitigate this with fixed prompts and manual validation of sampled cases.
    \item \textbf{Temporal dynamics}: We do not study how system performance changes as the case index grows over time.
\end{itemize}

\section*{Ethics Statement}

Our evaluation uses data from a private community with appropriate access permissions. All user identifiers are anonymized before analysis. The system includes an abstention mechanism to prevent answering questions beyond its knowledge, though automation bias remains a concern in production deployments. We do not release the proprietary evaluation data due to privacy considerations.

\section*{Data Availability}

The proprietary evaluation dataset cannot be released due to the sensitive nature of the technical discussions and privacy considerations for community members. We provide detailed statistics and methodology to enable reproducibility with similar data sources.

\bibliography{references}

\end{document}
