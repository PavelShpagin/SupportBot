#!/usr/bin/env python3
"""
Local ingestion runner.

Runs the full case-extraction pipeline on the sample_chat.json fixture
(augmented with a synthetic image message), then optionally POSTs to prod.

Usage:
    python3 run_local_ingest.py              # POST to prod, get working links
    python3 run_local_ingest.py --local      # In-memory only, links are local IDs (404 on prod)
    python3 run_local_ingest.py --local --post-to-prod  # Local processing + POST to prod for working links
"""
from __future__ import annotations

import base64
import hashlib
import json
import os
import time
import re
import struct
import sys
import uuid
import zlib
import subprocess
import tempfile
from pathlib import Path
from typing import List
from unittest.mock import MagicMock

ROOT = Path(__file__).resolve().parent
REPO_ROOT = ROOT.parent  # when script lives in legacy/, parent = repo root

# â”€â”€ sys.path â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
for d in [str(REPO_ROOT / "signal-ingest"), str(REPO_ROOT / "signal-bot")]:
    if d not in sys.path:
        sys.path.insert(0, d)

for lib in ("chromadb", "google", "google.generativeai", "mysql", "mysql.connector",
            "mysql.connector.errors"):
    if lib not in sys.modules:
        sys.modules[lib] = MagicMock()

# â”€â”€ env â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
env_path = REPO_ROOT / ".env"
if env_path.exists():
    for line in env_path.read_text().splitlines():
        line = line.strip()
        if line and not line.startswith("#") and "=" in line:
            k, _, v = line.partition("=")
            os.environ.setdefault(k.strip(), v.strip().strip('"'))

API_KEY   = os.environ.get("GOOGLE_API_KEY", "")
MODEL     = os.environ.get("MODEL_BLOCKS", "gemini-3.1-pro-preview")
MODEL_IMG = os.environ.get("MODEL_IMG", "gemini-3.1-pro-preview")
PROD_URL  = "https://supportbot.info"

if not API_KEY:
    sys.exit("GOOGLE_API_KEY not set")

# â”€â”€ generate a real PNG (320Ã—120 error-screen mockup) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _make_png(width: int, height: int, pixels_rgb) -> bytes:
    def chunk(tag, data):
        c = struct.pack('>I', len(data)) + tag + data
        return c + struct.pack('>I', zlib.crc32(tag + data) & 0xffffffff)
    raw = b''
    for row in pixels_rgb:
        raw += b'\x00'
        for r, g, b in row:
            raw += bytes([r, g, b])
    ihdr = struct.pack('>IIBBBBB', width, height, 8, 2, 0, 0, 0)
    idat = zlib.compress(raw)
    return (b'\x89PNG\r\n\x1a\n'
            + chunk(b'IHDR', ihdr)
            + chunk(b'IDAT', idat)
            + chunk(b'IEND', b''))

W, H = 320, 120
pixels = []
for y in range(H):
    row = []
    for x in range(W):
        if y < 30:
            row.append((30, 30, 80))    # title bar
        elif y < 35:
            row.append((200, 200, 200)) # separator
        elif 40 < y < 70 and 10 < x < 310:
            row.append((220, 50, 50))   # red error box
        elif 75 < y < 95 and 10 < x < 200:
            row.append((240, 240, 240)) # text area
        else:
            row.append((245, 245, 245))
    pixels.append(row)

_PNG_BYTES = _make_png(W, H, pixels)
_PNG_B64   = base64.b64encode(_PNG_BYTES).decode()
print(f"Generated PNG: {len(_PNG_BYTES)} bytes")

# â”€â”€ load fixture â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Use FIXTURE env var, or prod_chat.json if it exists, else sample_chat.json
_prod_fixture = REPO_ROOT / "tests" / "fixtures" / "prod_chat.json"
_sample_fixture = REPO_ROOT / "tests" / "fixtures" / "sample_chat.json"
_fixture_env = os.environ.get("FIXTURE")

if _fixture_env:
    fixture_path = Path(_fixture_env)
elif _prod_fixture.exists():
    fixture_path = _prod_fixture
    print(f"Using prod fixture: {fixture_path}")
else:
    fixture_path = _sample_fixture

data = json.loads(fixture_path.read_text(encoding="utf-8"))
messages = data if isinstance(data, list) else data.get("messages", [])

# Use GROUP_ID env or "group-x" for test runs (avoids long fixture group_id)
group_id = os.environ.get("GROUP_ID", "group-x")

_has_real_images = any(m.get("_image_payloads") for m in messages)

if not _has_real_images:
    # â”€â”€ inject synthetic image conversation (only when no real images in fixture) â”€â”€
    _base_ts    = messages[-1]["ts"] + 10_000
    _img_sender = messages[0]["sender"]
    _adm_sender = messages[1]["sender"]

    _img_id = "local-img-" + uuid.uuid4().hex[:8]
    _adm_id = "local-adm-" + uuid.uuid4().hex[:8]
    _cnf_id = "local-cnf-" + uuid.uuid4().hex[:8]

    _img_msg = {
        "ts": _base_ts,
        "sender": _img_sender,
        "sender_name": "Alpha User",
        "body": "Screensharing issues â€“ black screen on startup [image]",
        "id": _img_id,
        "reactions": 0,
        "attachments": [{"path": "attachments.noindex/local/error_screen.png",
                         "contentType": "image/png", "fileName": "error_screen.png"}],
        "_image_payload": {"filename": "error_screen.png",
                           "content_type": "image/png", "data_b64": _PNG_B64},
    }
    _adm_msg = {
        "ts": _base_ts + 3_000,
        "sender": _adm_sender,
        "sender_name": "Beta Admin",
        "body": "Restart the display service: sudo systemctl restart display-manager. This clears the init error.",
        "id": _adm_id,
        "reactions": 1,
        "reaction_emoji": "ðŸ‘",
    }
    _cnf_msg = {
        "ts": _base_ts + 6_000,
        "sender": _img_sender,
        "sender_name": "Alpha User",
        "body": "Worked! Thanks.",
        "id": _cnf_id,
        "reactions": 0,
    }
    messages = messages + [_img_msg, _adm_msg, _cnf_msg]

messages_augmented = list(messages)

# â”€â”€ LLM clients â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
os.environ.setdefault("SIGNAL_BOT_E164", "+10000000000")
os.environ.setdefault("DB_BACKEND", "mysql")
os.environ["SIGNAL_LISTENER_ENABLED"] = "false"
os.environ["USE_SIGNAL_DESKTOP"] = "false"

from openai import OpenAI
oc = OpenAI(
    api_key=API_KEY,
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/",
    timeout=120.0,  # Prevent indefinite hang on chunk 2 (Gemini API)
)

from app.llm.client import LLMClient
_s = MagicMock()
_s.openai_api_key  = API_KEY
_s.model_case      = MODEL
_s.embedding_model = os.getenv("EMBEDDING_MODEL", "gemini-embedding-001")
llm = LLMClient(_s)

from ingest.main import _chunk_messages, _ocr_attachment, _extract_structured_cases, _dedup_cases_llm

# â”€â”€ OCR all image messages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _enrich_with_ocr(msg: dict) -> None:
    """OCR all images in a message and append observations to body."""
    payloads = msg.get("_image_payloads") or []
    # Also support legacy singular _image_payload
    if not payloads and msg.get("_image_payload"):
        payloads = [msg["_image_payload"]]
    if not payloads:
        return
    for p in payloads:
        try:
            img_bytes = base64.b64decode(p["data_b64"])
        except Exception:
            continue
        ocr_json = _ocr_attachment(
            openai_client=oc, model=MODEL_IMG,
            image_bytes=img_bytes, content_type=p.get("content_type", "image/png"),
            context_text=msg.get("body") or "",
        )
        if ocr_json:
            try:
                ocr_data = json.loads(ocr_json)
                extracted_text = ocr_data.get("extracted_text") or ""
                observations   = ocr_data.get("observations") or []
                parts = []
                if extracted_text:
                    parts.append(f"Ð¢ÐµÐºÑÑ‚ Ð½Ð° Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð½Ñ–: {extracted_text}")
                if observations:
                    parts.append(f"Ð•Ð»ÐµÐ¼ÐµÐ½Ñ‚Ð¸ Ð½Ð° Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð½Ñ–: {', '.join(observations)}")
                if parts:
                    msg["body"] = (msg.get("body") or "") + "\n\n[Ð—Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð½Ñ: " + " | ".join(parts) + "]"
            except Exception:
                pass

img_msgs = [m for m in messages_augmented if m.get("_image_payloads") or m.get("_image_payload")]
print(f"\n>> OCR-ing {len(img_msgs)} image message(s)...")
for i, msg in enumerate(img_msgs, 1):
    print(f"  [{i}/{len(img_msgs)}] msg_id={msg['id']} ...", end=" ", flush=True)
    _enrich_with_ocr(msg)
    print(f"body now {len(msg.get('body',''))} chars")

# â”€â”€ Phase 1: chunk + extract structured cases (8x fewer API calls) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_chunk_max = int(os.environ.get("HISTORY_CHUNK_MAX_CHARS", "45000"))
_chunk_overlap = int(os.environ.get("HISTORY_CHUNK_OVERLAP_MESSAGES", "1"))
print(f"\n>> Chunking {len(messages_augmented)} messages (max_chars={_chunk_max})...")
chunks = _chunk_messages(messages=messages_augmented, max_chars=_chunk_max,
                         overlap_messages=_chunk_overlap, bot_e164="")
print(f"  â†’ {len(chunks)} chunk(s)")

print("\n>> Extracting structured cases (LLM)...")
all_structured: List[dict] = []
worker_script = ROOT / "extract_chunk_worker.py"
for i in range(len(chunks)):
    if i > 0:
        time.sleep(5)  # Pause between chunks to avoid API connection reuse hangs
    print(f"  chunk {i+1}/{len(chunks)}...", end=" ", flush=True)
    with tempfile.NamedTemporaryFile(mode="w", suffix=".json", delete=False, encoding="utf-8") as tf:
        json.dump({"chunk_text": chunks[i], "api_key": API_KEY, "model": MODEL, "structured": True}, tf, ensure_ascii=False)
        tmp_path = tf.name
    try:
        result = subprocess.run(
            [sys.executable, str(worker_script), tmp_path],
            capture_output=True,
            text=True,
            cwd=str(REPO_ROOT),
            timeout=600,
            env={**os.environ, "GOOGLE_API_KEY": API_KEY},
        )
        if result.returncode != 0:
            raise RuntimeError(f"Worker failed: {result.stderr or result.stdout}")
        cases = json.loads(result.stdout)
        all_structured.extend(cases)
        print(f"{len(cases)} case(s)")
    finally:
        os.unlink(tmp_path)

# LLM dedup
print("  dedup (LLM)...", end=" ", flush=True)
deduped = _dedup_cases_llm(openai_client=oc, model=MODEL, cases=all_structured)
print(f"{len(deduped)} case(s)")

# If using synthetic fixture and the image case wasn't extracted, inject it manually
if not _has_real_images:
    img_found = any(_img_id in c.get("case_block", "") or "display-manager" in (c.get("solution_summary") or "") for c in deduped)
    if not img_found:
        print("  âš  Image case not extracted â€” injecting manually")
        _img_hash = hashlib.sha256(_img_sender.encode()).hexdigest()[:16]
        _adm_hash = hashlib.sha256(_adm_sender.encode()).hexdigest()[:16]
        manual_case = {
            "keep": True,
            "status": "solved",
            "problem_title": "Ð§Ð¾Ñ€Ð½Ð¸Ð¹ ÐµÐºÑ€Ð°Ð½ Ð¿Ñ–Ð´ Ñ‡Ð°Ñ Ð·Ð°Ð¿ÑƒÑÐºÑƒ Ð´ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð°Ñ†Ñ–Ñ— ÐµÐºÑ€Ð°Ð½Ð°",
            "problem_summary": "Screensharing issues â€“ black screen on startup.",
            "solution_summary": "Restart the display service: sudo systemctl restart display-manager.",
            "tags": ["display-manager", "screenshare", "black-screen"],
            "evidence_ids": [_img_id, _adm_id, _cnf_id],
            "case_block": (
                f"{_img_hash} ts={_base_ts} msg_id={_img_id} reactions=0\n"
                f"{_img_msg['body']}\n\n"
                f"{_adm_hash} ts={_base_ts+3000} msg_id={_adm_id} reactions=1 reaction_emoji=ðŸ‘\n"
                f"{_adm_msg['body']}\n\n"
                f"{_img_hash} ts={_base_ts+6000} msg_id={_cnf_id} reactions=0\n"
                f"{_cnf_msg['body']}"
            ),
        }
        deduped.append(manual_case)

print(f"  â†’ {len(deduped)} structured cases total")

# â”€â”€ Local-only mode: batch embed, DB dedup (no make_case per block) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if "--local" in sys.argv:
    import sqlite3
    from contextlib import contextmanager

    class _LocalCursor:
        def __init__(self, cur): self._cur = cur
        def execute(self, sql, params=()):
            sql = sql.replace("%s", "?")
            if "JOIN case_evidence ce ON ce.case_id = c.case_id" in sql:
                sql = """UPDATE cases SET closed_emoji = ?, updated_at = CURRENT_TIMESTAMP
                    WHERE case_id IN (SELECT ce.case_id FROM case_evidence ce WHERE ce.message_id = ?)
                    AND status IN ('solved','open') AND closed_emoji IS NULL"""
            self._cur.execute(sql, params)
            self.rowcount = self._cur.rowcount
        def fetchone(self): return self._cur.fetchone()
        def fetchall(self): return self._cur.fetchall()

    class _LocalConn:
        def __init__(self, conn): self._conn = conn
        def cursor(self): return _LocalCursor(self._conn.cursor())
        def commit(self): self._conn.commit()
        def rollback(self): self._conn.rollback()

    class FakeDB:
        def __init__(self):
            self._conn = sqlite3.connect(":memory:", check_same_thread=False)
            self._conn.row_factory = sqlite3.Row
            self._conn.create_function("NOW", 0, lambda: "2000-01-01 00:00:00")
            c = self._conn.cursor()
            c.executescript("""
                CREATE TABLE cases (case_id TEXT PRIMARY KEY, group_id TEXT NOT NULL, status TEXT,
                problem_title TEXT, problem_summary TEXT, solution_summary TEXT, tags_json TEXT,
                evidence_image_paths_json TEXT, in_rag INTEGER DEFAULT 0, closed_emoji TEXT,
                embedding_json TEXT, created_at TIMESTAMP, updated_at TIMESTAMP);
                CREATE TABLE case_evidence (case_id TEXT, message_id TEXT, PRIMARY KEY(case_id,message_id));
                CREATE TABLE raw_messages (message_id TEXT PRIMARY KEY, group_id TEXT, ts INTEGER, sender_hash TEXT, sender_name TEXT, content_text TEXT, image_paths_json TEXT, reply_to_id TEXT);
            """)
            self._conn.commit()

        @contextmanager
        def connection(self):
            yield _LocalConn(self._conn)

        def all_cases(self):
            cur = self._conn.cursor()
            cur.execute("SELECT * FROM cases ORDER BY created_at")
            return [dict(r) for r in cur.fetchall()]

    from app.db.queries_mysql import upsert_case, find_similar_case, merge_case, store_case_embedding

    db = FakeDB()
    group_id = os.environ.get("GROUP_ID", "group-x")
    inserted = updated = 0
    # Batch embed (1 API call for all cases)
    embed_texts = [f"{c.get('problem_title','')}\n{c.get('problem_summary','')}" for c in deduped]
    embeddings = llm.embed_batch(texts=embed_texts)
    for i, case in enumerate(deduped):
        evidence_ids = list(case.get("evidence_ids") or [])
        if not evidence_ids:
            for line in (case.get("case_block") or "").split("\n"):
                m = re.search(r"msg_id=(\S+)", line)
                if m:
                    evidence_ids.append(m.group(1))
        similar_id = find_similar_case(db, group_id=group_id, embedding=embeddings[i])
        if similar_id:
            merge_case(db, target_case_id=similar_id, status=case.get("status", "solved"),
                problem_summary=case.get("problem_summary", ""), solution_summary=case.get("solution_summary", "") or "",
                tags=case.get("tags") or [], evidence_ids=evidence_ids, evidence_image_paths=[])
            store_case_embedding(db, similar_id, embeddings[i])
            updated += 1
        else:
            cid = uuid.uuid4().hex
            final_id, created = upsert_case(db, case_id=cid, group_id=group_id, status=case.get("status", "solved"),
                problem_title=case.get("problem_title", ""), problem_summary=case.get("problem_summary", ""),
                solution_summary=case.get("solution_summary", "") or "", tags=case.get("tags") or [],
                evidence_ids=evidence_ids, evidence_image_paths=[])
            store_case_embedding(db, final_id, embeddings[i])
            inserted += 1 if created else 0
            updated += 0 if created else 1

    final_cases = db.all_cases()
    print(f"\n{'='*65}")
    print(f"LOCAL PIPELINE RESULT: {len(messages_augmented)} messages -> {len(deduped)} structured cases")
    print(f"  Inserted: {inserted}  Merged: {updated}")
    print(f"  FINAL CASES: {len(final_cases)}")
    print(f"{'='*65}")
    for i, c in enumerate(final_cases, 1):
        emoji = f" [{c.get('closed_emoji')}]" if c.get("closed_emoji") else ""
        cid = c.get("case_id", "")
        print(f"  [{i}] {PROD_URL}/case/{cid}  [{c.get('status','')}{emoji}] {c.get('problem_title','')[:45]}")
    if "--post-to-prod" not in sys.argv:
        print(f"       (local IDs â€” add --post-to-prod for working prod links)")
        print(f"{'='*65}\n")
        sys.exit(0)
    print("  Posting to prod for working links...")
    print(f"{'='*65}\n")

# â”€â”€ Build messages payload (what gets posted to /history/cases) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _sender_hash(s: str) -> str:
    return hashlib.sha256(s.encode()).hexdigest()[:16]

messages_payload = []
for msg in messages_augmented:
    text = msg.get("body") or ""
    img_payloads = []
    # Support both plural (_image_payloads) and legacy singular (_image_payload)
    raw_payloads = msg.get("_image_payloads") or []
    if not raw_payloads and msg.get("_image_payload"):
        raw_payloads = [msg["_image_payload"]]
    for p in raw_payloads:
        img_payloads.append({
            "filename": p["filename"],
            "content_type": p["content_type"],
            "data_b64": p["data_b64"],
        })
    if not text and not img_payloads:
        continue
    messages_payload.append({
        "message_id": msg["id"],
        "sender_hash": _sender_hash(msg["sender"]),
        "sender_name": msg.get("sender_name"),
        "ts": msg["ts"],
        "content_text": text,
        "image_payloads": img_payloads,
    })

# â”€â”€ POST to prod â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import urllib.request

print(f"\n>> Creating debug token on prod...")
token_req = json.dumps({"admin_id": "local-test", "group_id": group_id}).encode()
req = urllib.request.Request(
    f"{PROD_URL}/history/token",
    data=token_req, headers={"Content-Type": "application/json"}, method="POST",
)
with urllib.request.urlopen(req) as r:
    token = json.load(r)["token"]
print(f"  token: {token[:16]}...")

print(f">> Posting {len(deduped)} structured cases + {len(messages_payload)} messages...")
history_req = json.dumps({
    "token": token,
    "group_id": group_id,
    "cases": [],  # Empty when using cases_structured (satisfies older prod schema)
    "cases_structured": [
        {
            "case_block": str(c.get("case_block") or ""),
            "problem_title": str(c.get("problem_title") or ""),
            "problem_summary": str(c.get("problem_summary") or ""),
            "solution_summary": str(c.get("solution_summary") or ""),
            "status": str(c.get("status") or "solved"),
            "tags": list(c.get("tags") or []) if isinstance(c.get("tags"), list) else [],
            "evidence_ids": list(c.get("evidence_ids") or []) if isinstance(c.get("evidence_ids"), list) else [],
        }
        for c in deduped
    ],
    "messages": messages_payload,
}).encode()
req2 = urllib.request.Request(
    f"{PROD_URL}/history/cases",
    data=history_req, headers={"Content-Type": "application/json"}, method="POST",
)
try:
    with urllib.request.urlopen(req2, timeout=120) as r:
        result = json.load(r)
except urllib.error.HTTPError as e:
    body = e.fp.read().decode() if e.fp else ""
    print(f"  HTTP {e.code}: {e.reason}")
    print(f"  Response: {body[:500]}")
    raise

print(f"  inserted: {result.get('cases_inserted')} / case_ids: {len(result.get('case_ids', []))}")

# â”€â”€ Report: fetch REAL case links from API (not from response order) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print(f"\n{'='*65}")
print(f"PIPELINE RESULT: {len(messages_augmented)} messages â†’ {len(chunks)} chunk(s)")
print(f"  Structured cases     : {len(deduped)}")
print(f"  Posted to prod       : {result.get('cases_inserted')} inserted")
print(f"{'='*65}")

# Fetch actual cases from API for this group â€” do not hallucinate or reuse IDs
try:
    cases_req = urllib.request.Request(
        f"{PROD_URL}/api/group-cases?group_id={group_id}&include_archived=true",
        headers={"Accept": "application/json"},
    )
    with urllib.request.urlopen(cases_req, timeout=15) as r:
        cases_data = json.load(r)
    cases_list = cases_data.get("cases", [])
    for i, c in enumerate(cases_list, 1):
        cid = c.get("case_id", "")
        title = (c.get("problem_title") or "")[:55]
        print(f"  [{i}] {PROD_URL}/case/{cid}  ({title})")
    # Mark image case if we have one with evidence containing the synthetic image
    img_case = next((c for c in cases_list if "display-manager" in (c.get("solution_summary") or "")
                    or "Screensharing" in (c.get("problem_title") or "")), None)
    if img_case:
        print(f"\n  [IMAGE] {PROD_URL}/case/{img_case.get('case_id', '')}")
except Exception as e:
    print(f"  (Could not fetch case links from API: {e})")
print(f"{'='*65}")
