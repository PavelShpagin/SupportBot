\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{amssymb}
\lstset{
    basicstyle=\ttfamily\small,
    breaklines=true,
    columns=fullflexible,
    keepspaces=true
}
\usepackage{fancyvrb}

\geometry{margin=1in}

\title{\textbf{SupportBot Multimodal Implementation Report}\\
\large Implementation Details, Evaluation Setup, and Examples}
\author{AI Agent (Cursor)}
\date{February 9, 2026}

\begin{document}

\maketitle

\begin{abstract}
This report documents the implementation of multimodal (text + images) support in SupportBot. It describes how image attachments are persisted, how images are passed into Gemini model calls via Google's OpenAI-compatible endpoint, and what evaluation scripts are included in the repository. Quantitative results depend on the availability of decrypted Signal history/attachments and API credentials; this document focuses on reproducible behavior verifiable from the codebase.
\end{abstract}

\tableofcontents
\newpage

%==============================================================================
\section{Executive Summary}
%==============================================================================

\subsection{Key Achievements}

\begin{table}[h]
\centering
\small
\begin{tabular}{p{4.2cm}p{9.8cm}}
\toprule
\textbf{Capability} & \textbf{Evidence in codebase} \\
\midrule
Persist image attachment paths & \texttt{raw\_messages.image\_paths\_json} (see \texttt{signal-bot/app/db/schema.py}, \texttt{signal-bot/app/db/schema\_mysql.py}) \\
Image-to-text extraction at ingestion & \texttt{signal-bot/app/ingestion.py} calling \texttt{LLMClient.image\_to\_text\_json()} \\
Pass images into gate/response LLM calls & \texttt{signal-bot/app/jobs/worker.py} passes images into \texttt{LLMClient.decide\_consider()} and \texttt{LLMClient.decide\_and\_respond()} \\
Store evidence image paths on cases & \texttt{signal-bot/app/jobs/worker.py:\_collect\_evidence\_image\_paths()} and DB field \texttt{cases.evidence\_image\_paths\_json} \\
Surface KB evidence images at response time & \texttt{signal-bot/app/jobs/worker.py} loads images from retrieved case metadata \texttt{evidence\_image\_paths} \\
\bottomrule
\end{tabular}
\caption{Multimodal capabilities implemented (verifiable in code)}
\end{table}

\subsection{Implementation Status}

All priority items from the proposed fix have been implemented:

\begin{itemize}
    \item[$\checkmark$] \textbf{P0}: Reject cases without solution\_summary (High impact)
    \item[$\checkmark$] \textbf{P1}: Pass images to \texttt{decide\_and\_respond()} (High impact)
    \item[$\checkmark$] \textbf{P2}: Pass images to \texttt{decide\_consider()} (Medium impact)
    \item[$\checkmark$] \textbf{P3}: Store image paths in \texttt{raw\_messages} (Enables P1/P2)
    \item[$\checkmark$] \textbf{P4}: Include images in KB case evidence (Medium impact)
\end{itemize}

%==============================================================================
\section{Algorithms (Current Implementation)}
%==============================================================================

This section presents pseudoalgorithms that mirror the current multimodal implementation in the codebase.

\subsection{Algorithm 1: Multimodal Message Ingestion}

\begin{algorithm}[H]
\caption{Multimodal Message Ingestion --- Preserves image paths for later use}
\begin{algorithmic}[1]
\Procedure{IngestMessage}{$msg\_id, group\_id, sender, ts, text, image\_paths$}
    \State $content\_text \gets text$
    \State $context\_text \gets text$
    \State $stored\_image\_paths \gets []$ \Comment{\textcolor{blue}{NEW: Track valid image paths}}
    \State
    \For{$path$ \textbf{in} $image\_paths$}
        \State $img\_path \gets \textsc{ResolveAttachmentPath}(path, settings.signal\_bot\_storage)$
        \State $img\_path \gets \textsc{Resolve}(img\_path)$
        \If{$\neg img\_path.\textsc{Exists}()$}
            \State \textsc{Log.Warning}(``Attachment missing: \{path\}'')
            \State \textbf{continue}
        \EndIf
        \State
        \State $stored\_image\_paths.\textsc{Append}(img\_path)$ \Comment{Store canonical path}
        \State
        \State \Comment{Extract text/observations for searchability}
        \State $img\_bytes \gets \textsc{ReadFile}(img\_path)$
        \State $extraction \gets \textsc{LLM.ImageToTextJSON}(img\_bytes, context\_text)$
        \State $content\_text \gets content\_text + \text{``[image]''} + \textsc{JSON}(extraction)$
        \State \Comment{On extraction error, store placeholder JSON \{observations: [], extracted\_text: ""\}}
    \EndFor
    \State
    \State \Comment{Store image paths alongside text}
    \State \textsc{InsertRawMessage}$(msg\_id, group\_id, ts, sha256(sender)[:16],$
    \Statex \hspace{8em} $content\_text, stored\_image\_paths, reply\_to)$
    \State
    \State \textsc{EnqueueJob}$(BUFFER\_UPDATE, \{group\_id, msg\_id\})$
    \State \textsc{EnqueueJob}$(MAYBE\_RESPOND, \{group\_id, msg\_id\})$
\EndProcedure
\end{algorithmic}
\end{algorithm}

\textbf{Notable behavior:}
\begin{itemize}
    \item Image paths are stored in the database for later multimodal calls
    \item Relative attachment paths are resolved against \texttt{SIGNAL\_BOT\_STORAGE}
    \item Image-to-text extraction output is appended to message text for searchability
\end{itemize}

\newpage
\subsection{Algorithm 2: Case Extraction with Validation}

\begin{algorithm}[H]
\caption{Case Extraction with Solution Validation --- Reject solved cases without solutions}
\begin{algorithmic}[1]
\Procedure{HandleBufferUpdate}{$group\_id, msg\_id$}
    \State $msg \gets \textsc{GetRawMessage}(msg\_id)$
    \State $line \gets \textsc{FormatBufferLine}(msg)$
    \State $buffer \gets \textsc{GetBuffer}(group\_id)$
    \State $buffer\_new \gets buffer + line$
    \State
    \State $extract \gets \textsc{LLM.ExtractCase}(buffer\_new)$
    \If{$\neg extract.found$}
        \State \textsc{SetBuffer}$(group\_id, buffer\_new)$
        \State \Return
    \EndIf
    \State
    \State $case \gets \textsc{LLM.MakeCase}(extract.case\_block)$
    \If{$\neg case.keep$}
        \State \textsc{SetBuffer}$(group\_id, extract.buffer\_new)$
        \State \Return
    \EndIf
    \State
    \State \Comment{Reject solved cases without solutions}
    \If{$case.status = \text{``solved''} \land case.solution\_summary.\textsc{Strip}() = \text{``''}$}
        \State \textsc{Log.Warning}(``Rejecting solved case without solution\_summary'')
        \State \textsc{SetBuffer}$(group\_id, extract.buffer\_new)$
        \State \Return
    \EndIf
    \State
    \State $case\_id \gets \textsc{NewUUID}()$
    \State
    \State \Comment{Collect image paths from evidence messages}
    \State $evidence\_image\_paths \gets \textsc{CollectEvidenceImages}(case.evidence\_ids)$
    \State
    \State \textsc{InsertCase}$(case\_id, group\_id, case.*, evidence\_image\_paths)$
    \State
    \State $doc\_text \gets \textsc{JoinLines}(case.problem\_title, case.problem\_summary,$
    \Statex \hspace{7em} $case.solution\_summary, \text{``tags: ''} + \textsc{Join}(case.tags))$
    \State $embedding \gets \textsc{LLM.Embed}(doc\_text)$
    \State
    \State \Comment{Store image paths in metadata for retrieval}
    \State \textsc{Chroma.Upsert}$(case\_id, doc\_text, embedding,$
    \Statex \hspace{6em} $\{group\_id, status, evidence\_ids, evidence\_image\_paths\})$
    \State
    \State \textsc{SetBuffer}$(group\_id, extract.buffer\_new)$
\EndProcedure
\State
\Procedure{CollectEvidenceImages}{$evidence\_ids$}
    \State $paths \gets []$
    \For{$msg\_id$ \textbf{in} $evidence\_ids$}
        \State $msg \gets \textsc{GetRawMessage}(msg\_id)$
        \If{$msg \neq \text{null}$}
            \For{$p$ \textbf{in} $msg.image\_paths$}
                \State $paths.\textsc{Append}(p)$
            \EndFor
        \EndIf
    \EndFor
    \State \Return $paths$
\EndProcedure
\end{algorithmic}
\end{algorithm}

\textbf{Notable behavior:}
\begin{itemize}
    \item Solved cases must have non-empty solution summaries
    \item Evidence image paths collected from raw messages
    \item Image paths stored in vector DB metadata for later retrieval
\end{itemize}

\newpage
\subsection{Algorithm 3: Multimodal Response Pipeline}

\begin{algorithm}[H]
\caption{Multimodal Response Pipeline --- Images at every decision point}
\begin{algorithmic}[1]
\Procedure{HandleMaybeRespond}{$group\_id, msg\_id$}
    \State $msg \gets \textsc{GetRawMessage}(msg\_id)$ \Comment{Now includes image\_paths}
    \State $context \gets \textsc{GetLastNMessages}(group\_id, n)$
    \State
    \State \Comment{Load images from current message for gate}
    \State $msg\_images \gets \textsc{LoadImages}(msg.image\_paths, max\_gate, budget)$
    \State
    \State $force \gets \textsc{MentionsBot}(msg.content\_text)$
    \If{$\neg force$}
        \State \Comment{Gate sees images}
        \State $decision \gets \textsc{LLM.DecideConsider}(msg.content\_text, context, msg\_images)$
        \If{$\neg decision.consider$}
            \State \Return \Comment{Ignore greeting/noise}
        \EndIf
    \EndIf
    \State
    \State $query\_embedding \gets \textsc{LLM.Embed}(msg.content\_text)$
    \State $retrieved \gets \textsc{Chroma.Retrieve}(group\_id, query\_embedding, k)$
    \State
    \State \Comment{Collect images from retrieved KB cases}
    \State $kb\_paths \gets []$
    \For{$item$ \textbf{in} $retrieved$}
        \State $paths \gets item.metadata.evidence\_image\_paths$
        \State $kb\_paths.\textsc{Extend}(paths[:max\_per\_case])$
    \EndFor
    \State $kb\_paths \gets kb\_paths[:max\_total\_kb]$
    \State
    \State \Comment{Load KB images (respecting budget after msg images)}
    \State $remaining\_budget \gets \textsc{Max}(budget - \textsc{TotalSize}(msg\_images), 0)$
    \State $kb\_images \gets \textsc{LoadImages}(kb\_paths, max\_respond, remaining\_budget)$
    \State
    \State $all\_images \gets msg\_images + kb\_images$
    \State $all\_images \gets all\_images[:max\_images\_per\_respond]$ \Comment{Final cap}
    \State
    \State $cases\_json \gets \textsc{JSON}(retrieved)$
    \State
    \State \Comment{Responder sees all images}
    \State $resp \gets \textsc{LLM.DecideAndRespond}(msg.content\_text, context,$
    \Statex \hspace{10em} $cases\_json, all\_images)$
    \State
    \If{$resp.respond$}
        \State \textsc{Signal.Send}$(group\_id, resp.text)$
    \EndIf
\EndProcedure
\State
\Procedure{LoadImages}{$paths, max\_count, budget\_bytes$}
    \State $images \gets []$
    \State $total \gets 0$
    \For{$p$ \textbf{in} $paths$}
        \If{$|images| \geq max\_count$}
            \State \textbf{break}
        \EndIf
        \State $data \gets \textsc{ReadFile}(p)$
        \State $size \gets |data|$
        \If{$size > max\_image\_size$}
            \State \textbf{continue}
        \EndIf
        \If{$total + size > budget\_bytes$}
            \State \textbf{break}
        \EndIf
        \State $mime \gets \textsc{GuessMimeType}(p)$
        \State $images.\textsc{Append}((data, mime))$
        \State $total \gets total + size$
    \EndFor
    \State \Return $images$
\EndProcedure
\end{algorithmic}
\end{algorithm}

\textbf{Notable behavior:}
\begin{itemize}
    \item \textbf{P2}: Gate stage receives images from user message
    \item \textbf{P1}: Responder receives images from both user message and KB evidence
    \item \textbf{P4}: Evidence images retrieved from case metadata
    \item Image budgets cap multimodal payload size (\texttt{MAX\_IMAGE\_SIZE\_BYTES} and \texttt{MAX\_TOTAL\_IMAGE\_BYTES})
\end{itemize}

\newpage
%==============================================================================
\section{Examples (Illustrative)}
%==============================================================================

This section provides illustrative, synthetic examples to show how messages are transformed into structured cases and how images flow through the pipeline. Message IDs, timestamps, similarity values, and file paths shown below are placeholders and will differ in real runs.

\subsection{Example 1: Compatibility Question (synthetic)}

\subsubsection{Raw Messages (Input)}

\begin{lstlisting}[basicstyle=\small\ttfamily,frame=none]
User:
Is device MODEL_X supported? Which target should I select when flashing?

Support:
Yes. Use target TARGET_Y. The tooling will detect it as TARGET_Y.
\end{lstlisting}

\subsubsection{Extracted Case Block}

\begin{Verbatim}[fontsize=\small]
CASE BLOCK:
problem: Device compatibility and flashing target selection
evidence: [msg_id_1, msg_id_2]
status: solved
\end{Verbatim}

\subsubsection{Structured Case (Output)}

\begin{table}[h]
\centering
\begin{tabular}{p{3cm}p{10cm}}
\toprule
\textbf{Field} & \textbf{Value} \\
\midrule
\texttt{case\_id} & \texttt{c4f2a891-...} \\
\texttt{status} & \texttt{solved} \\
\texttt{problem\_title} & Support for device MODEL\_X \\
\texttt{problem\_summary} & User asks whether MODEL\_X is supported and what target to select when flashing. \\
\texttt{solution\_summary} & Support confirms compatibility and recommends selecting target TARGET\_Y in the flashing tool. \\
\texttt{tags} & compatibility, firmware, flashing, target selection \\
\texttt{evidence\_ids} & [msg\_id\_1, msg\_id\_2] \\
\texttt{evidence\_image\_paths} & \texttt{[]} (no images in this case) \\
\bottomrule
\end{tabular}
\caption{Structured case with metadata}
\end{table}

\subsubsection{Embedding \& Storage}

\begin{itemize}
    \item \textbf{Document text}: Built from title + summaries + tags (see \texttt{signal-bot/app/jobs/worker.py})
    \item \textbf{Embedding}: Vector produced by the configured \texttt{EMBEDDING\_MODEL}
    \item \textbf{Vector DB}: Stored in ChromaDB with metadata: \texttt{\{group\_id, status, evidence\_ids, evidence\_image\_paths\}}
\end{itemize}

\newpage
\subsection{Example 2: Multimodal Message with Screenshot (synthetic)}

\subsubsection{Raw Messages (Input)}

\begin{Verbatim}[fontsize=\small]
User:
Please help. I see an error when trying to complete an action. What does it mean?
[image: <attachment.png>]

Support:
Which mode/step are you in when the error appears?

User:
It happens in MODE\_A. In MODE\_B it works normally.

Support:
Please share logs/config. The screenshot may include an error code we can diagnose.
\end{Verbatim}

\subsubsection{Structured Case with Image Paths}

\begin{table}[h]
\centering
\small
\begin{tabular}{p{3.5cm}p{9.5cm}}
\toprule
\textbf{Field} & \textbf{Value} \\
\midrule
\texttt{problem\_title} & Error shown in screenshot during MODE\_A \\
\texttt{problem\_summary} & User cannot complete an action in MODE\_A; screenshot contains an error message/code. MODE\_B works. \\
\texttt{solution\_summary} & Gather logs/config and troubleshoot based on the exact error text visible in the screenshot and retrieved evidence cases. \\
\texttt{evidence\_image\_paths} & \textcolor{blue}{\texttt{["<abs\_path\_to\_attachment.png>"]}} \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{How Images Are Used}

\textbf{At ingestion:}
\begin{itemize}
    \item Image extracted to text: \texttt{\{observations: ["..."], extracted\_text: "..."\}}
    \item Image path stored: \texttt{<abs\_path\_to\_attachment.png>}
\end{itemize}

\textbf{At retrieval (when user asks similar question):}
\begin{enumerate}
    \item User query: "What does this screenshot error mean?"
    \item System retrieves relevant case(s) via semantic similarity search
    \item Loads evidence image(s) from disk (bounded by size/budget limits)
    \item Passes image(s) + retrieved case text to the LLM for response generation
\end{enumerate}

\newpage
%==============================================================================
\section{Solved Cases: Retrieval Introspection}
%==============================================================================

This section demonstrates how the bot retrieves and reasons about cases when answering user questions, with full introspection into the retrieval pipeline.

\subsection{Example Query 1: Gimbal Control Issue}

\subsubsection{User Question}

\begin{Verbatim}[fontsize=\small]
User: Need to control FEATURE\_X but it doesn't work on this configuration. 
It looks like DEVICE\_Y lacks MODE\_Z. RC passthrough alone is not enough.
\end{Verbatim}

\subsubsection{Stage 1: Semantic Search}

\textbf{Query embedding:} Generated from user question\\
\textbf{Search parameters:}
\begin{itemize}
    \item \texttt{group\_id}: \texttt{<group\_id>}
    \item \texttt{k}: \texttt{RETRIEVE\_TOP\_K} (default: 5)
    \item \texttt{embedding\_model}: \texttt{EMBEDDING\_MODEL} (configurable)
\end{itemize}

\textbf{Retrieved cases (ranked by similarity score):}

\begin{table}[h]
\centering
\small
\begin{tabular}{cl}
\toprule
\textbf{Rank} & \textbf{Case Title} \\
\midrule
1 & FEATURE\_X control troubleshooting \\
2 & Output/servo mode configuration \\
3 & Missing MODE\_Z support on DEVICE\_Y \\
4 & RC passthrough configuration \\
5 & Firmware option enabling FEATURE\_X \\
\bottomrule
\end{tabular}
\caption{Top-5 retrieved cases}
\end{table}

\subsubsection{Stage 2: Image Loading}

\textbf{For each retrieved case:}
\begin{itemize}
    \item Case 1: \texttt{evidence\_image\_paths} = \texttt{[]} (no images)
    \item Case 2: \texttt{evidence\_image\_paths} = \texttt{[]} (no images)
    \item Case 3: \texttt{evidence\_image\_paths} = \texttt{[]} (no images)
    \item Case 4: \texttt{evidence\_image\_paths} = \texttt{[]} (no images)
    \item Case 5: \texttt{evidence\_image\_paths} = \texttt{[]} (no images)
\end{itemize}

\textbf{Total images loaded:} \texttt{<n>}\\
\textbf{Total budget used:} \texttt{<bytes>} / \texttt{MAX\_TOTAL\_IMAGE\_BYTES}

\subsubsection{Stage 3: LLM Decision}

\textbf{Input to LLM:}
\begin{itemize}
    \item User message (text)
    \item Context: last 40 messages from group
    \item Retrieved cases: JSON with top-5 cases
    \item Images: 0 message images + 0 KB images = 0 total
\end{itemize}

\textbf{LLM output:}
\begin{Verbatim}[fontsize=\small]
{
  "respond": true,
  "text": "<answer based on retrieved case evidence>",
  "citations": ["case:<case_id>"]
}
\end{Verbatim}

\newpage
\subsection{Example Query 2: Configuration Parameter Question}

\subsubsection{User Question}

\begin{Verbatim}[fontsize=\small]
User: Good day. Analyzing parameter "PARAM\_X": Value: V. If the system error 
exceeds this threshold, a failsafe will trigger. Is this too low? What are the recommendations?
\end{Verbatim}

\subsubsection{Retrieval Results}

\begin{table}[h]
\centering
\small
\begin{tabular}{clc}
\toprule
\textbf{Rank} & \textbf{Case Title} & \textbf{Status} \\
\midrule
1 & PARAM\_X configuration & solved \\
2 & Failsafe parameters & solved \\
3 & Sensor/estimation troubleshooting & solved \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Top case evidence:}
\begin{itemize}
    \item \textbf{Problem}: User asks about a configuration parameter and whether the current value is appropriate
    \item \textbf{Solution}: \texttt{<solution summary from retrieved case evidence>}
    \item \textbf{Evidence IDs}: [msg\_id\_42, msg\_id\_43, msg\_id\_44]
    \item \textbf{Images}: No images in evidence
\end{itemize}

\subsubsection{Bot Response}

\begin{Verbatim}[fontsize=\small]
<answer based on retrieved case evidence>

Ref: case:<case_id>...
\end{Verbatim}

\newpage
\subsection{Example Query 3: Multimodal Response (with Screenshot)}

\subsubsection{User Question with Image}

\begin{Verbatim}[fontsize=\small]
User: Look at what the ground control app shows. Don't understand what 
kind of error this is. [image: <attachment.png>]
\end{Verbatim}

\subsubsection{Image Processing Pipeline}

\textbf{Step 1: Message ingestion}
\begin{itemize}
    \item Image path stored in DB: \texttt{<abs\_path\_to\_attachment.png>}
    \item Image-to-text JSON appended to \texttt{content\_text}: \texttt{\{"observations": ["..."], "extracted\_text": "..."\}}
\end{itemize}

\textbf{Step 2: Gate decision (\texttt{decide\_consider})}
\begin{itemize}
    \item Message text: "Look at what the ground control app shows..."
    \item \textbf{Images loaded:} up to \texttt{MAX\_IMAGES\_PER\_GATE} (subject to size/budget limits)
    \item LLM sees: text + the attached image(s)
    \item Decision: \texttt{consider=<true/false>}
\end{itemize}

\textbf{Step 3: Semantic retrieval}
\begin{itemize}
    \item Query embedding from: \texttt{<user message text>}
    \item Top retrieved case: \texttt{<case title>}
    \item Case may include \texttt{evidence\_image\_paths}: \texttt{["<abs\_path\_to\_evidence.png>"]}
\end{itemize}

\textbf{Step 4: Response generation (\texttt{decide\_and\_respond})}
\begin{itemize}
    \item \textbf{Message images:} \texttt{<n>}
    \item \textbf{KB images:} \texttt{<n>}
    \item \textbf{Total images:} capped by \texttt{MAX\_IMAGES\_PER\_RESPOND}
    \item \textbf{Total bytes:} capped by \texttt{MAX\_TOTAL\_IMAGE\_BYTES}
    \item LLM sees: user image(s) + retrieved case text (+ optional KB evidence image(s))
\end{itemize}

\subsubsection{Bot Response}

\begin{Verbatim}[fontsize=\small]
<answer based on retrieved case evidence>

Ref: case:<case_id>...
\end{Verbatim}

\textbf{Why multimodal can help (conceptually):}
\begin{itemize}
    \item Without images: the model only sees text, which may be ambiguous
    \item With images: the model can directly see UI text/error codes and other visual context
    \item With KB evidence images: the model can compare the user's screenshot to historical evidence
\end{itemize}

\newpage
%==============================================================================
\section{Evaluation and Reproducibility}
%==============================================================================
\label{sec:evaluation-and-reproducibility}

This repository contains tests and evaluation scripts. Most evaluation outputs are written under \texttt{test/data/} and are intentionally gitignored; quantitative metrics therefore depend on the local environment, available decrypted Signal history/attachments, and API credentials.

\subsection{Unit tests (offline)}
\begin{itemize}
    \item Run: \texttt{pytest -v}
    \item Core coverage includes ingestion, buffer/case extraction, Chroma integration, and response gating.
\end{itemize}

\subsection{LLM-backed quality evaluation (requires \texttt{GOOGLE\_API\_KEY})}
\begin{itemize}
    \item Tests: \texttt{test/test\_quality\_eval.py}
    \item The judge uses Gemini via Google's OpenAI-compatible endpoint.
\end{itemize}

\subsection{Real-data evaluation (optional; requires decrypted Signal history)}
\begin{itemize}
    \item Mine cases: \texttt{python test/mine\_real\_cases.py}
    \item Run eval: \texttt{python test/run\_real\_quality\_eval.py}
    \item Streaming eval: \texttt{python test/run\_streaming\_eval.py}
    \item Image-to-text demo: \texttt{python test/run\_image\_to\_text\_demo.py}
\end{itemize}

\subsection{Notes on Signal Desktop encryption}
Signal Desktop backups may require Windows DPAPI decryption under the same Windows user account that created the backup (see \texttt{test/results.md} for a documented example).

\subsection{Embedding model note}
\begin{itemize}
    \item Application default: \texttt{EMBEDDING\_MODEL=text-embedding-004} (see \texttt{signal-bot/app/config.py}).
    \item The real-eval script may override to \texttt{gemini-embedding-001} for compatibility with the OpenAI-style Gemini embeddings endpoint (see \texttt{test/run\_real\_quality\_eval.py}).
\end{itemize}

\newpage
%==============================================================================
\section{Configuration and Limits}
%==============================================================================

\subsection{Multimodal Settings}

\begin{table}[h]
\centering
\begin{tabular}{lrl}
\toprule
\textbf{Parameter} & \textbf{Value} & \textbf{Purpose} \\
\midrule
\texttt{MAX\_IMAGES\_PER\_GATE} & 3 & Limit images sent to gate decision \\
\texttt{MAX\_IMAGES\_PER\_RESPOND} & 5 & Limit total images in response call \\
\texttt{MAX\_KB\_IMAGES\_PER\_CASE} & 2 & Limit evidence images per retrieved case \\
\texttt{MAX\_IMAGE\_SIZE\_BYTES} & 5,000,000 & Skip images $>$ 5,000,000 bytes \\
\texttt{MAX\_TOTAL\_IMAGE\_BYTES} & 20,000,000 & Total budget per response (20,000,000 bytes) \\
\bottomrule
\end{tabular}
\caption{Image budget limits (caps multimodal payload size)}
\label{tab:image-budget-limits}
\end{table}

\subsection{Cost considerations}

The main cost drivers are LLM calls during ingestion and response:
\begin{itemize}
    \item \textbf{Ingestion}: optional image-to-text extraction per attachment (\texttt{image\_to\_text\_json})
    \item \textbf{Retrieval}: embeddings for case documents and user queries (\texttt{embed})
    \item \textbf{Response}: gate (\texttt{decide\_consider}) and responder (\texttt{decide\_and\_respond}) chat calls, optionally with images
\end{itemize}

Actual costs depend on model selection and provider pricing. The implementation enforces strict caps on image count and total bytes to bound multimodal payload size (Table~\ref{tab:image-budget-limits}).

%==============================================================================
\section{Conclusion}
%==============================================================================

This implementation adds end-to-end multimodal plumbing:

\begin{enumerate}
    \item \textbf{Reject low-quality cases} (P0): Reject \texttt{status=solved} cases without \texttt{solution\_summary}
    \item \textbf{Preserve image references} (P3): Store attachment paths in \texttt{raw\_messages.image\_paths\_json}
    \item \textbf{Use images in decisions and responses} (P1, P2): Pass images into \texttt{decide\_consider} and \texttt{decide\_and\_respond}
    \item \textbf{Carry evidence images through retrieval} (P4): Store and retrieve \texttt{evidence\_image\_paths} via Chroma metadata
\end{enumerate}

\textbf{Measuring impact:} Use the scripts in Section~\ref{sec:evaluation-and-reproducibility} to run evaluations in an environment with decrypted data and valid API credentials.

\textbf{Next steps:}
\begin{itemize}
    \item Deploy to production and monitor real-world performance
    \item Gather user feedback on response quality
    \item Fine-tune retrieval thresholds based on precision/recall metrics
    \item Consider adding image captioning for better searchability
\end{itemize}

\end{document}
