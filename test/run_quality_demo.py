#!/usr/bin/env python3
"""
Quality demonstration script for SupportBot.

Shows real examples of bot behavior with Gemini evaluation.

Run (recommended):
  - Put `GOOGLE_API_KEY=...` in `.env` (repo root), OR export it in your shell
  - `python test/run_quality_demo.py`
"""

import json
import os
import sys
from pathlib import Path
from typing import List, Dict, Optional
from dataclasses import dataclass

sys.path.insert(0, str(Path(__file__).parent.parent / "signal-bot"))

def _maybe_load_dotenv(dotenv_path: Path) -> None:
    """
    Load key=value pairs from .env, stripping CRLF, without overriding existing env.
    """
    if not dotenv_path.exists():
        return
    for raw in dotenv_path.read_text(encoding="utf-8", errors="ignore").splitlines():
        line = raw.strip()
        if not line or line.startswith("#") or "=" not in line:
            continue
        k, v = line.split("=", 1)
        k = k.strip()
        v = v.strip().strip("\r")
        if not k:
            continue
        if (v.startswith("'") and v.endswith("'")) or (v.startswith('"') and v.endswith('"')):
            v = v[1:-1]
        os.environ.setdefault(k, v)


_maybe_load_dotenv(Path(__file__).resolve().parent.parent / ".env")

if not os.environ.get("GOOGLE_API_KEY"):
    print("ERROR: GOOGLE_API_KEY environment variable not set")
    print("Put GOOGLE_API_KEY in .env or export it, then rerun.")
    sys.exit(1)

from openai import OpenAI
from app.llm.client import LLMClient
from app.config import Settings


# =============================================================================
# Setup
# =============================================================================

def create_settings() -> Settings:
    return Settings(
        db_backend="mysql",
        mysql_host="localhost",
        mysql_port=3306,
        mysql_user="test",
        mysql_password="test",
        mysql_database="test",
        oracle_user="",
        oracle_password="",
        oracle_dsn="",
        oracle_wallet_dir="",
        openai_api_key=os.environ["GOOGLE_API_KEY"],
        model_img="gemini-2.0-flash",
        model_decision="gemini-2.5-flash-lite",
        model_extract="gemini-2.5-flash-lite",
        model_case="gemini-2.5-flash-lite",
        model_respond="gemini-2.0-flash",
        model_blocks="gemini-2.0-flash",
        embedding_model="text-embedding-004",
        chroma_url="http://localhost:8001",
        chroma_collection="test",
        signal_bot_e164="+10000000000",
        signal_bot_storage="/tmp",
        signal_ingest_storage="/tmp",
        signal_cli="signal-cli",
        bot_mention_strings=["@supportbot"],
        signal_listener_enabled=False,
        log_level="WARNING",
        context_last_n=40,
        retrieve_top_k=5,
        worker_poll_seconds=1,
        history_token_ttl_minutes=60,
        max_images_per_gate=3,
        max_images_per_respond=5,
        max_kb_images_per_case=2,
        max_image_size_bytes=5_000_000,
        max_total_image_bytes=20_000_000,
    )


# =============================================================================
# Knowledge Base
# =============================================================================

KNOWLEDGE_BASE = [
    {
        "case_id": "case-001",
        "problem": "–ù–µ–º–æ–∂–ª–∏–≤—ñ—Å—Ç—å —É–≤—ñ–π—Ç–∏ –≤ –æ—Å–æ–±–∏—Å—Ç–∏–π –∫–∞–±—ñ–Ω–µ—Ç",
        "solution": "–°–∫–∏–Ω—É—Ç–∏ –ø–∞—Ä–æ–ª—å —á–µ—Ä–µ–∑ —Ñ–æ—Ä–º—É –≤—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è –Ω–∞ —Å—Ç–æ—Ä—ñ–Ω—Ü—ñ –≤—Ö–æ–¥—É.",
        "tags": ["login", "password"],
    },
    {
        "case_id": "case-002", 
        "problem": "–í—ñ–¥–µ–æ —É—Ä–æ–∫–∏ –Ω–µ –∑–∞–≤–∞–Ω—Ç–∞–∂—É—é—Ç—å—Å—è –≤ Firefox",
        "solution": "–í–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ Chrome –∞–±–æ Edge. Firefox –º–∞—î –ø—Ä–æ–±–ª–µ–º–∏ —Å—É–º—ñ—Å–Ω–æ—Å—Ç—ñ.",
        "tags": ["video", "browser"],
    },
    {
        "case_id": "case-003",
        "problem": "–û—Ç—Ä–∏–º–∞–Ω–Ω—è —Å–µ—Ä—Ç–∏—Ñ—ñ–∫–∞—Ç—É",
        "solution": "–ó–∞–≤–µ—Ä—à–∏—Ç–∏ –≤—Å—ñ –º–æ–¥—É–ª—ñ —Ç–∞ —Å–∫–ª–∞—Å—Ç–∏ —Ç–µ—Å—Ç –Ω–∞ 70%+. –°–µ—Ä—Ç–∏—Ñ—ñ–∫–∞—Ç –≤: –ö–∞–±—ñ–Ω–µ—Ç ‚Üí –ú–æ—ó —Å–µ—Ä—Ç–∏—Ñ—ñ–∫–∞—Ç–∏.",
        "tags": ["certificate"],
    },
    {
        "case_id": "case-004",
        "problem": "–ù–µ–º–∞—î –¥–æ—Å—Ç—É–ø—É –ø—ñ—Å–ª—è –æ–ø–ª–∞—Ç–∏",
        "solution": "–ù–∞–ø–∏—Å–∞—Ç–∏ –≤ –ø—ñ–¥—Ç—Ä–∏–º–∫—É –∑ –Ω–æ–º–µ—Ä–æ–º —Ç—Ä–∞–Ω–∑–∞–∫—Ü—ñ—ó. –ê–∫—Ç–∏–≤—É—é—Ç—å –≤—Ä—É—á–Ω—É.",
        "tags": ["payment"],
    },
    {
        "case_id": "case-005",
        "problem": "–ú–æ–±—ñ–ª—å–Ω–∏–π –¥–æ–¥–∞—Ç–æ–∫",
        "solution": "–î–æ–¥–∞—Ç–æ–∫ '–°—Ç–∞–±–• –ê–∫–∞–¥–µ–º—ñ—è' –≤ App Store/Google Play. –û—Ñ–ª–∞–π–Ω –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–æ—Å—Ç—É–ø–Ω–µ.",
        "tags": ["mobile", "app"],
    },
]


# =============================================================================
# Demo
# =============================================================================

def run_demo():
    print("=" * 80)
    print("SUPPORTBOT QUALITY DEMONSTRATION")
    print("=" * 80)
    print()
    
    settings = create_settings()
    llm = LLMClient(settings)
    
    # Judge client
    judge = OpenAI(
        api_key=os.environ["GOOGLE_API_KEY"],
        base_url="https://generativelanguage.googleapis.com/v1beta/openai/",
    )
    
    def format_cases(cases: List[Dict]) -> str:
        return json.dumps([{
            "case_id": c["case_id"],
            "document": f"{c['problem']}\n{c['solution']}",
            "metadata": {"group_id": "test", "status": "solved"},
        } for c in cases], ensure_ascii=False)
    
    def evaluate_response(question: str, response: str, cases: List[Dict]) -> Dict:
        kb = "\n".join([f"{c['case_id']}: {c['solution']}" for c in cases])
        prompt = f"""Evaluate this Ukrainian support bot response.

KNOWLEDGE: {kb}
QUESTION: {question}
RESPONSE: {response}

Return JSON: {{"score": 0-10, "accurate": bool, "helpful": bool, "hallucination": bool, "note": "..."}}"""
        
        resp = judge.chat.completions.create(
            model="gemini-2.5-flash-lite",
            messages=[{"role": "user", "content": prompt}],
            response_format={"type": "json_object"},
            temperature=0,
        )
        return json.loads(resp.choices[0].message.content or "{}")
    
    # =========================================================================
    # SECTION 1: Questions that SHOULD get answers
    # =========================================================================
    
    print("‚ïî" + "‚ïê" * 78 + "‚ïó")
    print("‚ïë SECTION 1: Questions that SHOULD get helpful answers                       ‚ïë")
    print("‚ïö" + "‚ïê" * 78 + "‚ïù")
    print()
    
    should_answer = [
        ("–ü—Ä–∏–≤—ñ—Ç, –Ω–µ –º–æ–∂—É –∑–∞–π—Ç–∏ –≤ –∫–∞–±—ñ–Ω–µ—Ç, –ø–∏—à–µ –Ω–µ–≤—ñ—Ä–Ω–∏–π –ø–∞—Ä–æ–ª—å", [KNOWLEDGE_BASE[0]]),
        ("–í—ñ–¥–µ–æ —É—Ä–æ–∫–∏ –Ω–µ –≤–∞–Ω—Ç–∞–∂–∞—Ç—å—Å—è, –∫—Ä—É—Ç–∏—Ç—å—Å—è –∫–æ–ª–µ—Å–æ –≤–∂–µ –≥–æ–¥–∏–Ω—É", [KNOWLEDGE_BASE[1]]),
        ("–ö–æ–ª–∏ —è –æ—Ç—Ä–∏–º–∞—é —Å–µ—Ä—Ç–∏—Ñ—ñ–∫–∞—Ç –ø—Ä–æ –ø—Ä–æ—Ö–æ–¥–∂–µ–Ω–Ω—è –∫—É—Ä—Å—É?", [KNOWLEDGE_BASE[2]]),
        ("–û–ø–ª–∞—Ç–∏–≤ –∫—É—Ä—Å, –≥—Ä–æ—à—ñ —Å–ø–∏—Å–∞–ª–∏—Å—å, –∞ –¥–æ—Å—Ç—É–ø—É –Ω–µ–º–∞—î!", [KNOWLEDGE_BASE[3]]),
        ("–ß–∏ —î –º–æ–±—ñ–ª—å–Ω–∏–π –¥–æ–¥–∞—Ç–æ–∫? –•–æ—á—É –≤ –º–µ—Ç—Ä–æ –¥–∏–≤–∏—Ç–∏—Å—è", [KNOWLEDGE_BASE[4]]),
    ]
    
    all_passed = True
    for question, cases in should_answer:
        print(f"üìù –ü–ò–¢–ê–ù–ù–Ø: {question}")
        
        # Stage 1
        decision = llm.decide_consider(message=question, context="–ì—Ä—É–ø–∞ —Ç–µ—Ö–ø—ñ–¥—Ç—Ä–∏–º–∫–∏ –∞–∫–∞–¥–µ–º—ñ—ó")
        
        if not decision.consider:
            print(f"   ‚ùå Stage 1: Bot ignored (WRONG!)")
            all_passed = False
            print()
            continue
        
        # Stage 2
        result = llm.decide_and_respond(
            message=question,
            context="–ì—Ä—É–ø–∞ —Ç–µ—Ö–ø—ñ–¥—Ç—Ä–∏–º–∫–∏",
            cases=format_cases(cases),
        )
        
        if not result.respond:
            print(f"   ‚ùå Stage 2: Bot declined (WRONG!)")
            all_passed = False
            print()
            continue
        
        response = result.text
        if result.citations:
            response += f" [–†–µ—Ñ: {', '.join(result.citations[:2])}]"
        
        print(f"ü§ñ –í–Ü–î–ü–û–í–Ü–î–¨: {response}")
        
        # Evaluate
        eval_result = evaluate_response(question, result.text, cases)
        score = eval_result.get("score", 0)
        accurate = eval_result.get("accurate", False)
        hallucination = eval_result.get("hallucination", False)
        
        if score >= 7 and accurate and not hallucination:
            print(f"   ‚úÖ Score: {score}/10 | Accurate: {accurate} | No hallucination")
        else:
            print(f"   ‚ö†Ô∏è Score: {score}/10 | Accurate: {accurate} | Hallucination: {hallucination}")
            print(f"      Note: {eval_result.get('note', '')}")
            all_passed = False
        
        print()
    
    # =========================================================================
    # SECTION 2: Questions that should be DECLINED (no knowledge)
    # =========================================================================
    
    print("‚ïî" + "‚ïê" * 78 + "‚ïó")
    print("‚ïë SECTION 2: Questions that should be DECLINED (no knowledge ‚Üí no answer)   ‚ïë")
    print("‚ïö" + "‚ïê" * 78 + "‚ïù")
    print()
    
    should_decline = [
        "–Ø–∫ –Ω–∞–ª–∞—à—Ç—É–≤–∞—Ç–∏ Kubernetes –∫–ª–∞—Å—Ç–µ—Ä?",
        "–ü–æ—Ä–µ–∫–æ–º–µ–Ω–¥—É–π—Ç–µ —Ö–æ—Ä–æ—à–∏–π —Ä–µ—Å—Ç–æ—Ä–∞–Ω —É –ö–∏—î–≤—ñ",
        "–Ø–∫–∞ –ø–æ–≥–æ–¥–∞ –±—É–¥–µ –∑–∞–≤—Ç—Ä–∞?",
        "–Ø–∫ –Ω–∞–ø–∏—Å–∞—Ç–∏ —Ä–µ–∫—É—Ä—Å–∏–≤–Ω—É —Ñ—É–Ω–∫—Ü—ñ—é –Ω–∞ Haskell?",
    ]
    
    for question in should_decline:
        print(f"üìù –ü–ò–¢–ê–ù–ù–Ø: {question}")
        
        decision = llm.decide_consider(message=question, context="–ì—Ä—É–ø–∞ —Ç–µ—Ö–ø—ñ–¥—Ç—Ä–∏–º–∫–∏ –∞–∫–∞–¥–µ–º—ñ—ó")
        
        if not decision.consider:
            print(f"   ‚úÖ Stage 1: Correctly ignored (irrelevant)")
            print()
            continue
        
        # Stage 2 - with NO cases
        result = llm.decide_and_respond(
            message=question,
            context="–ì—Ä—É–ø–∞ —Ç–µ—Ö–ø—ñ–¥—Ç—Ä–∏–º–∫–∏ –∞–∫–∞–¥–µ–º—ñ—ó",
            cases="[]",  # Empty!
        )
        
        if not result.respond:
            print(f"   ‚úÖ Stage 2: Correctly declined (no evidence)")
        else:
            print(f"   ‚ùå HALLUCINATION! Bot answered: {result.text[:100]}...")
            all_passed = False
        
        print()
    
    # =========================================================================
    # SECTION 3: Messages that should be IGNORED (greetings, noise)
    # =========================================================================
    
    print("‚ïî" + "‚ïê" * 78 + "‚ïó")
    print("‚ïë SECTION 3: Messages that should be IGNORED (greetings, noise)             ‚ïë")
    print("‚ïö" + "‚ïê" * 78 + "‚ïù")
    print()
    
    should_ignore = [
        "–ü—Ä–∏–≤—ñ—Ç –≤—Å—ñ–º!",
        "–î–æ–±—Ä–æ–≥–æ —Ä–∞–Ω–∫—É)",
        "–æ–∫ –¥—è–∫—É—é",
        "üëç",
        "–Ø–∫ —Å–ø—Ä–∞–≤–∏?",
        "+1",
        "–ó–≥–æ–¥–µ–Ω",
    ]
    
    for message in should_ignore:
        print(f"üí¨ –ü–û–í–Ü–î–û–ú–õ–ï–ù–ù–Ø: {message}")
        
        decision = llm.decide_consider(message=message, context="–ì—Ä—É–ø–∞ —Ç–µ—Ö–ø—ñ–¥—Ç—Ä–∏–º–∫–∏")
        
        if not decision.consider:
            print(f"   ‚úÖ Correctly ignored")
        else:
            print(f"   ‚ùå Should have ignored this!")
            all_passed = False
        
        print()
    
    # =========================================================================
    # SECTION 4: Conciseness check
    # =========================================================================
    
    print("‚ïî" + "‚ïê" * 78 + "‚ïó")
    print("‚ïë SECTION 4: Conciseness check (responses should be brief)                  ‚ïë")
    print("‚ïö" + "‚ïê" * 78 + "‚ïù")
    print()
    
    test_q = "–Ø–∫ —Å–∫–∏–Ω—É—Ç–∏ –ø–∞—Ä–æ–ª—å?"
    result = llm.decide_and_respond(
        message=test_q,
        context="–ì—Ä—É–ø–∞ —Ç–µ—Ö–ø—ñ–¥—Ç—Ä–∏–º–∫–∏",
        cases=format_cases([KNOWLEDGE_BASE[0]]),
    )
    
    if result.respond:
        length = len(result.text)
        print(f"üìù –ü–∏—Ç–∞–Ω–Ω—è: {test_q}")
        print(f"ü§ñ –í—ñ–¥–ø–æ–≤—ñ–¥—å ({length} —Å–∏–º–≤–æ–ª—ñ–≤):")
        print(f"   {result.text}")
        
        if length <= 200:
            print(f"   ‚úÖ Good length ({length} chars)")
        elif length <= 400:
            print(f"   ‚ö†Ô∏è Acceptable ({length} chars)")
        else:
            print(f"   ‚ùå Too long ({length} chars)")
            all_passed = False
    
    print()
    
    # =========================================================================
    # SUMMARY
    # =========================================================================
    
    print("=" * 80)
    if all_passed:
        print("‚úÖ ALL CHECKS PASSED")
        print()
        print("Summary:")
        print("  ‚Ä¢ Bot answers correctly when it has knowledge")
        print("  ‚Ä¢ Bot stays silent when it doesn't know (no hallucinations)")
        print("  ‚Ä¢ Bot ignores greetings and noise (no false alerts)")
        print("  ‚Ä¢ Responses are in Ukrainian")
        print("  ‚Ä¢ Responses are concise and helpful")
    else:
        print("‚ö†Ô∏è SOME CHECKS FAILED - review output above")
    print("=" * 80)


if __name__ == "__main__":
    run_demo()
